{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#===================================================#\n",
    "# This program applies intertextual phrase matching #\n",
    "# to one or more files in the following formats:    #\n",
    "#   - plain python strings, tuples, lists           #\n",
    "#   - plain text files                              #\n",
    "#   - csv-files (TODO)                              #\n",
    "#   - TEI XML files (internally converted to TF)    #\n",
    "#   - Text-Fabric file packages                     #\n",
    "#                                                   #\n",
    "# Author: Ernst Boogert                             #\n",
    "# Institution: Protestant Theological University    #\n",
    "# Place: Amsterdam, the Netherlands                 #\n",
    "#                                                   #\n",
    "# Version: 0.0.8 (stable version)                   #\n",
    "# Last modified: July 8th, 2019                     #\n",
    "#===================================================#\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "from pandas import DataFrame\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tf.fabric import Fabric, Timestamp\n",
    "from collatex import Collation, collate\n",
    "from itertools import combinations_with_replacement as selfcombinations\n",
    "from itertools import combinations, product\n",
    "\n",
    "#local imports\n",
    "from helpertools.tokenizer import splitWord, tokenize, strip_accents\n",
    "\n",
    "tm = Timestamp()\n",
    "\n",
    "class FlexGrams:\n",
    "    def __init__(self, base_path, comp_path=None, ngram=5, \n",
    "                 skip=1, ngram_type=\"ordered\", start=0, stop=0, steps=1,\n",
    "                 mode=0, distance_base=0, distance_comp=0, \n",
    "                 number=1, context=0, sort_order=\"base\", self_match=False,\n",
    "                 stopwords=None, synonyms=None):\n",
    "        # External variables\n",
    "        self.base_path     = base_path\n",
    "        self.comp_path     = comp_path\n",
    "        self.ngram         = ngram\n",
    "        self.skip          = skip\n",
    "        self.ngram_type    = ngram_type\n",
    "        self.start         = start\n",
    "        self.stop          = stop\n",
    "        self.steps         = steps\n",
    "        self.mode          = mode\n",
    "        self.distance_base = distance_base\n",
    "        self.distance_comp = distance_comp\n",
    "        self.number        = number\n",
    "        self.context       = context\n",
    "        self.sort_order    = sort_order\n",
    "        self.self_match    = self_match\n",
    "        self.stopwords     = stopwords         # Should be a set\n",
    "        self.synonyms      = synonyms          # Should be a dict\n",
    "        # Internal Variables\n",
    "        self.base_sources  = OrderedDict()\n",
    "        self.comp_sources  = OrderedDict()\n",
    "        self.tf_apis_dict  = {}\n",
    "        self.source_count  = 0\n",
    "        self.result_dict   = self.ipm()\n",
    "        \n",
    "    \n",
    "    def path_loader(self, path):\n",
    "        sources_dict = {}\n",
    "        \n",
    "        # Helper functions that return action upon path\n",
    "        def walk_file(file):\n",
    "            name, ext = os.path.splitext(file)\n",
    "            if ext == '.xml':\n",
    "                pass # TODO: apply TEI xml converter\n",
    "            elif ext == '.txt':\n",
    "                sources_dict[f'{self.source_count}_{name}'] = \\\n",
    "                    [word for word in open(file, 'r').read().split()]\n",
    "                self.source_count +=1\n",
    "            elif ext == '':\n",
    "                try:\n",
    "                    sources_dict[f'{self.source_count}_{name}'] = \\\n",
    "                        [word for word in open(file, 'r').read().split()]\n",
    "                    self.source_count +=1\n",
    "                except:\n",
    "                    print(\"No valid input...!\")\n",
    "                        \n",
    "        def walk_dir(path):\n",
    "            if os.path.isdir(path):\n",
    "                if os.path.isfile(path + '/oslots.tf'):\n",
    "                    self.tf_apis_dict[f'TF_{self.source_count}'] = self.tf_initiate(path)\n",
    "                    sources_dict[f'TF_{self.source_count}'] = \\\n",
    "                        self.tf_mode(self.tf_apis_dict[f'TF_{self.source_count}'])\n",
    "                    self.source_count +=1\n",
    "                else:\n",
    "                    with os.scandir(path) as it:\n",
    "                        for entry in it:\n",
    "                            if entry.is_dir():\n",
    "                                walk_dir(entry.path)\n",
    "                            elif entry.is_file():\n",
    "                                walk_file(entry.path)\n",
    "                            else:\n",
    "                                print('Something went wrong while walking the dir...')\n",
    "                                \n",
    "        # If it is a single file: try to read and to tokenize it\n",
    "        if os.path.isfile(path):    \n",
    "            walk_file(path) \n",
    "        # If it is a dir: scan the dirs and files inside\n",
    "        elif os.path.isdir(path):   \n",
    "            walk_dir(path)  \n",
    "        # If it is a string: tokenize the string    \n",
    "        elif isinstance(path, str): \n",
    "            sources_dict[f'{self.source_count}_string'] = self.non_tf_mode(path)\n",
    "            self.source_count +=1\n",
    "        # If it is a tuple or list: pass it on to the algoritm\n",
    "        elif isinstance(path, (list, tuple)): \n",
    "            sources_dict[f'{self.source_count}_list'] = [path]\n",
    "            self.source_count +=1\n",
    "        else:\n",
    "            print('Input error: are you sure that your input is valid?')\n",
    "        tm.info(f'{self.source_count} source text(s) found...')\n",
    "        return sources_dict\n",
    "        \n",
    "    \n",
    "    def non_tf_mode(self, text):\n",
    "        ''' Explanation modes:\n",
    "        In all modes, standard normalization will be applied...\n",
    "        0 = exact comparison\n",
    "        1 = unaccented exact comparison\n",
    "        2 = accented lemmatized comparison\n",
    "        3 = unaccented lemmatized comparison\n",
    "        4 = betacode accented comparison\n",
    "        5 = betacode unaccented comparison\n",
    "        '''\n",
    "        modes = {\n",
    "                0: tokenize(text),\n",
    "                1: [strip_accents(i) for i in tokenize(text)],\n",
    "                2: None,\n",
    "        }\n",
    "        return modes[self.mode]\n",
    "    \n",
    "    def tf_mode(self, api): #TODO!\n",
    "        ''' Explanation modes:\n",
    "        In all modes, standard normalization will be applied...\n",
    "        0 = exact comparison\n",
    "        1 = unaccented exact comparison\n",
    "        2 = accented lemmatized comparison\n",
    "        3 = unaccented lemmatized comparison\n",
    "        4 = betacode accented comparison\n",
    "        5 = betacode unaccented comparison\n",
    "        '''\n",
    "        if self.stop == 0:\n",
    "            stop = api.F.otype.s('word')[-1] #api.F.otype.maxSlot\n",
    "        else:\n",
    "            stop = self.stop\n",
    "        if self.start == 0:\n",
    "            start = api.F.otype.s('word')[0]\n",
    "        # The format of the list comprehensions is necessary to ensure that for each slot something is returned, \n",
    "        # even if it is empty, because otherwise slot numbers will not correspond with the internal numbers\n",
    "        modes = { \n",
    "                0: [api.T.text(token, fmt='text-orig-main') for token in range(start, stop)], # normalized original without punctuation\n",
    "                1: [api.T.text(token, fmt='text-orig-full') for token in range(start, stop)], # original form \n",
    "                2: [api.T.text(token, fmt='text-orig-plain') for token in range(start, stop)], #tf main-regularized-unicode-unaccented\n",
    "                3: [api.T.text(token, fmt='text-orig-lemma') for token in range(start, stop)], #tf main-lemmatized-accented\n",
    "                4: [],#tf main-lemmatized-unaccented\n",
    "                5: [api.T.text(token, fmt='text-orig-beta') for token in range(start, stop)],#tf main-betacode-accented\n",
    "                6: [api.T.text(token, fmt='text-orig-beta-plain') for token in range(start, stop)],#tf main-betacode-unaccented\n",
    "        }\n",
    "#         print(modes[self.mode])\n",
    "        return modes[self.mode] # returns a list of words, to be fed into the ngrams_generator\n",
    "\n",
    "    \n",
    "    def ngrams_generator(self, source, text):\n",
    "        '''The ngrams_generator is a modified version of the\n",
    "        ITERTOOLS: combinations() function.'''\n",
    "        # Setting up the parameters\n",
    "        ngram = self.ngram\n",
    "        skip = self.skip\n",
    "        if self.ngram_type == \"ordered\":\n",
    "            ngram_type = tuple\n",
    "        elif self.ngram_type == \"unordered\":\n",
    "            ngram_type = frozenset\n",
    "        start = self.start\n",
    "        if self.stop == 0:\n",
    "            stop = len(text)\n",
    "        else:\n",
    "            stop = self.stop\n",
    "        steps = self.steps\n",
    "\n",
    "        #Doing the loop and output ngrams in format (startindex, (word1, word2, word3, ...))\n",
    "        while start <= stop - ngram:\n",
    "            ngrams = text[(start):(start + ngram)]\n",
    "            length_ngrams = len(ngrams)                     # Calculate the length of the ngram_pool\n",
    "            result = ngram - skip                           # Calculate the length of the result\n",
    "            indices = list(range(result))                   # Creates the initial index to calculate the startpoint of the resulting ngrams\n",
    "            yield (start, ngram_type(ngrams[i] for i in indices))\n",
    "\n",
    "            while True:\n",
    "                for i in reversed(range(result)):\n",
    "                    if indices[i] != i + length_ngrams - result:  # Checks whether the result is not equal to the initial resulting ngram\n",
    "                        break\n",
    "                indices[i] +=1                           # Updates all index items by one\n",
    "                for j in range(i+1, result):\n",
    "                    indices[j] = indices[j-1] + 1  \n",
    "                if indices[0] > 0:                       # If the skip becomes the first element, the algorithm goes to the next ngram to prevent duplicates\n",
    "                    start += steps\n",
    "                    break\n",
    "                yield (start, ngram_type(ngrams[i] for i in indices))\n",
    "\n",
    "    \n",
    "    def ngrams_dict(self, sources_dict):\n",
    "        ngrams_dict = OrderedDict()\n",
    "        for source, text in sources_dict.items():\n",
    "            ngrams_dict[source] = defaultdict(list)\n",
    "            for (start, ngram) in self.ngrams_generator(source, text):\n",
    "                ngrams_dict[source][ngram].append(start)\n",
    "#         pprint(ngrams_dict)\n",
    "        return ngrams_dict\n",
    "\n",
    "    \n",
    "    def matcher(self, base_ngrams_dict, comp_ngrams_dict=None):\n",
    "        '''The matcher function returns a dict\n",
    "        with matched sources as keys in a tuple: (source1, source2)\n",
    "        and the matching ngram slices as list of tuples: [(1, 5), (8, 16), ...]'''\n",
    "        #TODO: implement fuzzywuzzy as alternative for skips, or implement set methods\n",
    "        match_dict = {} #OrderedDict() #defaultdict(list)\n",
    "                \n",
    "        if comp_ngrams_dict == None: # If this is the case, the algorithm conducts a selfcomparison...\n",
    "            if len(base_ngrams_dict) == 1: # In this case, the algorithm expects a selfmatch, even is self.self_match is false (=default)\n",
    "                key = next(iter(base_ngrams_dict))\n",
    "                match_dict[(key, key)] = \\\n",
    "                    sorted([ (i, i+self.ngram, j, j+self.ngram, 1) for v in base_ngrams_dict[key].values() \\\n",
    "                                                                   for i in v[0:len(v)-1] \\\n",
    "                                                                   for j in v[1:len(v)] if j > i + self.ngram ])\n",
    "            else: #base_ngrams_dict contains more than one source\n",
    "                if self.self_match == False:\n",
    "                    dictloop = combinations(base_ngrams_dict.keys(), 2)\n",
    "                else:\n",
    "                    dictloop = selfcombinations(base_ngrams_dict.keys(), 2) # Returns all source combinations including selfmatch\n",
    "                for loop1, loop2 in dictloop:\n",
    "                    if loop1 == loop2: # = selfmatch!\n",
    "                        match_dict[(loop1, loop2)] = \\\n",
    "                            sorted([ (i, i+self.ngram, j, j+self.ngram, 1) for v in base_ngrams_dict[loop1].values() \\\n",
    "                                                                           for i in v[0:len(v)-1] \\\n",
    "                                                                           for j in v[1:len(v)] \\\n",
    "                                                                           if j > i + self.ngram ])\n",
    "                    else:\n",
    "                        match_dict[(loop1, loop2)] = \\\n",
    "                            sorted([ (i, i+self.ngram, j, j+self.ngram, 1) for k1, v1 in base_ngrams_dict[loop1].items() \\\n",
    "                                                                           for i in v1 \\\n",
    "                                                                           for j in base_ngrams_dict[loop2][k1] \\\n",
    "                                                                           if k1 in base_ngrams_dict[loop2] ])\n",
    "                                                              \n",
    "        else:\n",
    "            dictloop = product(base_ngrams_dict.keys(), comp_ngrams_dict.keys())\n",
    "            for loop1, loop2 in dictloop:\n",
    "                match_dict[(loop1, loop2)] = \\\n",
    "                    sorted([ (ib, ib+self.ngram, ic, ic+self.ngram, 1) for kc, vc in comp_ngrams_dict[loop2].items() \\\n",
    "                                                                    for ic in vc \\\n",
    "                                                                    for ib in base_ngrams_dict[loop1][kc] \\\n",
    "                                                                    if kc in base_ngrams_dict[loop1]])\n",
    "            \n",
    "#         pprint(match_dict)\n",
    "        return match_dict\n",
    "        \n",
    "\n",
    "    def concatenator(self, match_dict):\n",
    "        \n",
    "        def worker(match_list):\n",
    "            if match_list != []:\n",
    "                cmatch_list = [match_list[0]]\n",
    "                for i in match_list[1:]:\n",
    "                    j = 1\n",
    "                    assigned = False\n",
    "                    # Check whether the base slice fits in the allowed range\n",
    "                    # Since the list is sorted, i[0] is always greater than the one before\n",
    "                    # First, we check whether the base start index number (=i[0]) fits within the allowed range...\n",
    "                    while i[0] <= cmatch_list[-j][1] + self.distance_base:\n",
    "                        # Then we check whether the comp start index number (=i[2]) fits with in the allowed range\n",
    "                        if i[2] >= cmatch_list[-j][2] - self.distance_comp - 1 and \\\n",
    "                           i[2] <= cmatch_list[-j][3] + self.distance_comp + 1:\n",
    "                            # If both checks are True, we process the concatenation...\n",
    "                            if i[2] >= cmatch_list[-j][2]:\n",
    "                                if i[3] > cmatch_list[-j][3]:\n",
    "                                    cmatch_list[-j] = tuple((cmatch_list[-j][0],\n",
    "                                                             i[1],\n",
    "                                                             cmatch_list[-j][2],\n",
    "                                                             i[3],\n",
    "                                                             cmatch_list[-j][4] + i[4] if not cmatch_list[-j][1] == i[1] else cmatch_list[-j][4], \n",
    "                                                             ))\n",
    "                                    assigned = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    cmatch_list[-j] = tuple((cmatch_list[-j][0],\n",
    "                                                             i[1],\n",
    "                                                             cmatch_list[-j][2],\n",
    "                                                             cmatch_list[-j][3],\n",
    "                                                             cmatch_list[-j][4], # + i[4] if not cmatch_list[-j][1] == i[1] else cmatch_list[-j][4], \n",
    "                                                             ))\n",
    "                                    assigned = True\n",
    "                                    break\n",
    "                             \n",
    "                            else: # i[2] <= cmatch_list[-j][2]]\n",
    "                                if i[3] < cmatch_list[-j][3]:\n",
    "                                    cmatch_list[-j] = tuple((cmatch_list[-j][0],\n",
    "                                                             i[1],\n",
    "                                                             i[2],\n",
    "                                                             cmatch_list[-j][3],\n",
    "                                                             cmatch_list[-j][4] + i[4] if not cmatch_list[-j][1] == i[1] else cmatch_list[-j][4], \n",
    "                                                             ))\n",
    "                                    assigned = True\n",
    "                                    break\n",
    "                                else: #i [3] >= cmatch_list[-j][3]\n",
    "                                    cmatch_list[-j] = tuple((cmatch_list[-j][0],\n",
    "                                                             i[1],\n",
    "                                                             i[2],\n",
    "                                                             i[3],\n",
    "                                                             cmatch_list[-j][4] + i[4] if not cmatch_list[-j][1] == i[1] else cmatch_list[-j][4], \n",
    "                                                             ))\n",
    "                                assigned = True\n",
    "                                break\n",
    "                            \n",
    "                        # If the comp slice does not fit: \n",
    "                        else:\n",
    "                            # Check one slice earlier\n",
    "                            j +=1\n",
    "                            if j > len(cmatch_list):\n",
    "                                if assigned == False:\n",
    "                                    cmatch_list.append(i)\n",
    "                                break\n",
    "                            continue\n",
    "                    else:\n",
    "                        if assigned == False:\n",
    "                            cmatch_list.append(i)\n",
    "#                 pprint(cmatch_list)\n",
    "            else:\n",
    "                cmatch_list = []\n",
    "                \n",
    "            if len(cmatch_list) != len(match_list):\n",
    "                return worker(cmatch_list)\n",
    "            else:\n",
    "                return cmatch_list\n",
    "#                 fcmatch_dict = {key: [tuple((i[0], i[1], i[2], i[3], i[0]-self.ngram)) for i in value] for key, value in cmatch_dict.items()}\n",
    "        for sm in match_dict:\n",
    "            match_dict[sm] = worker(match_dict[sm])\n",
    "        fmatch_dict = {}\n",
    "        for sm in match_dict:\n",
    "            fmatch_dict[sm] = [s for s in match_dict[sm] if s[4] >= self.number]\n",
    "#         pprint(fmatch_dict)\n",
    "        return fmatch_dict\n",
    "\n",
    "    def context_concatenator(self):\n",
    "        '''Concatenates matches that fall within the context parameter'''\n",
    "        # Not finished!!!\n",
    "        contextualized_match_dict = {}\n",
    "        return contextualized_match_dict\n",
    "        \n",
    "        \n",
    "    def ipm(self):\n",
    "        # Load all tokenized packages in base_path in base_sources dictionary\n",
    "        # Send base_path to path_loader --> path_loader() uses mode_tokenizer() to decide how to tokenize\n",
    "        self.base_sources = self.path_loader(self.base_path) #Get a dict with one or more base texts\n",
    "        base_ngrams_dict = self.ngrams_dict(self.base_sources)\n",
    "        if self.comp_path == None:\n",
    "            match_dict = self.matcher(base_ngrams_dict)\n",
    "            fcmatch_dict = self.concatenator(match_dict)\n",
    "\n",
    "        # Load all tokenized packages in comp_path one by one into memory,\n",
    "        # execute comparison and delete them from memory subsequently\n",
    "        # Walk through comp_path and send them one by one to the path_loader()\n",
    "        # Path(loader) returns a dict with the name of the package (=key) and a tokenized list (=value)\n",
    "        else:\n",
    "            #Execute comparison between base and comp\n",
    "#           for file in self.comp_path: # TODO!!! Convert to os.path expression and make them load one by one\n",
    "            self.comp_sources = self.path_loader(self.comp_path)\n",
    "            comp_ngrams_dict = self.ngrams_dict(self.comp_sources)\n",
    "            match_dict = self.matcher(base_ngrams_dict, comp_ngrams_dict)\n",
    "            fcmatch_dict = self.concatenator(match_dict)\n",
    "#         pprint(fcmatch_dict)\n",
    "        return fcmatch_dict\n",
    "\n",
    "    def getRefList(self, api, levels='all'):\n",
    "        nodeType = api.TF.features['otext'].metaData['availableStructure'].split(',')[-1]\n",
    "        sectionNodes = api.F.otype.s(nodeType)\n",
    "#         refList = [\".\".join(str(i[1]) for i in api.T.headingFromNode(node)[1:]) for node in sectionNodes]\n",
    "        refs = [tuple(str(i[1]) for i in api.T.headingFromNode(node)[1:]) for node in sectionNodes]\n",
    "        refList = list(OrderedDict.fromkeys(refs))\n",
    "#         list(OrderedDict.fromkeys( self.getRefList(api_b, levels='all') ))\n",
    "        if levels == 'all':\n",
    "            return ['.'.join(ref) for ref in refList]\n",
    "        else:\n",
    "            return ['.'.join([ref[level] for level in levels]) for ref in refList]\n",
    "            \n",
    "    \n",
    "    def refResult(self, order='base', b_levels='all', c_levels='all'):\n",
    "        frame_dict = OrderedDict(bibl_start=[], bibl_stop=[],\n",
    "                                 patr_start=[], patr_stop=[],\n",
    "                                 typ=[], conf=[], \n",
    "                                 source=[], found=[],\n",
    "                                 base_text=[], comp_text=[])\n",
    "        result_dict = self.result_dict\n",
    "        for sm in result_dict: # sm = source match = tuple of two documents that have matching ngrams\n",
    "            if sm[0].startswith('TF_'):\n",
    "                api_b = self.tf_apis_dict[sm[0]]\n",
    "                b_min, b_max = api_b.F.otype.s('word')[0], api_b.F.otype.s('word')[-1]\n",
    "            if sm[1].startswith('TF_'):\n",
    "                api_c = self.tf_apis_dict[sm[1]]\n",
    "                c_min, c_max = api_c.F.otype.s('word')[0], api_c.F.otype.s('word')[-1]\n",
    "                \n",
    "            for match in result_dict[sm]:    \n",
    "                if sm[0].startswith('TF_'):\n",
    "                    # Definition of references\n",
    "                    b_start = [ str(i[1]) for i in api_b.T.headingFromNode(api_b.L.u(match[0]+1, otype=api_b.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    b_stop = [ str(i[1]) for i in api_b.T.headingFromNode(api_b.L.u(match[1], otype=api_b.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    c_start = [ str(i[1]) for i in api_c.T.headingFromNode(api_c.L.u(match[2]+1, otype=api_c.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    c_stop = [ str(i[1]) for i in api_c.T.headingFromNode(api_c.L.u(match[3], otype=api_c.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    \n",
    "                    con = 5\n",
    "                    base_start = match[0] - con if not match[0] - con < b_min else b_min\n",
    "                    base_stop = match[1] + con if not match[1] + con > b_max else b_max\n",
    "                    comp_start = match[2] - con if not match[2] - con < c_min else c_min\n",
    "                    comp_stop = match[3] + con if not match[3] + con > c_max else c_max\n",
    "                    \n",
    "                frame_dict['bibl_start'].append('.'.join(b_start)) if b_levels == 'all' \\\n",
    "                    else frame_dict['bibl_start'].append( '.'.join([b_stop[level] for level in b_levels]) ) \n",
    "                frame_dict['bibl_stop'].append('.'.join(b_stop)) if b_levels == 'all' \\\n",
    "                    else frame_dict['bibl_stop'].append( '.'.join([b_start[level] for level in b_levels]) ) \n",
    "                frame_dict['patr_start'].append('.'.join(c_start)) if c_levels == 'all' \\\n",
    "                    else frame_dict['patr_start'].append( '.'.join([c_start[level] for level in c_levels]) ) \n",
    "                frame_dict['patr_stop'].append('.'.join(c_stop)) if c_levels == 'all' \\\n",
    "                    else frame_dict['patr_stop'].append( '.'.join([c_stop[level] for level in c_levels]) )\n",
    "                frame_dict['typ'].append('undefined')    \n",
    "                frame_dict['conf'].append('U')\n",
    "                frame_dict['source'].append('Flexgrams')\n",
    "                frame_dict['found'].append('False')\n",
    "                frame_dict['base_text'].append( ' '.join([ token for token in api_b.T.text(range(base_start, base_stop), fmt='text-orig-full', descend=True).split() ]).strip() )\n",
    "                frame_dict['comp_text'].append( ' '.join([ token for token in api_c.T.text(range(comp_start, comp_stop), fmt='text-orig-full', descend=True).split() ]).strip() )\n",
    "                    \n",
    "        data = DataFrame(frame_dict)\n",
    "        \n",
    "        # Build refLists\n",
    "        refListBase = self.getRefList(api_b, levels='all') \n",
    "        refListComp = self.getRefList(api_c, levels=(0, 2, 3)) \n",
    "        sortIndexBase = dict(zip(refListBase, range(len(refListBase))))\n",
    "        sortIndexComp = dict(zip(refListComp, range(len(refListComp))))\n",
    "        \n",
    "        # Add sort columns\n",
    "        data['r_bibl_start'] = data['bibl_start'].map(sortIndexBase)\n",
    "        data['r_patr_start'] = data['patr_start'].map(sortIndexComp)\n",
    "        if order == 'base':\n",
    "            data.sort_values(['r_bibl_start', 'r_patr_start', 'patr_stop', 'bibl_stop'], \\\n",
    "                              ascending = [True, True, True, True], inplace=True)\n",
    "        else:\n",
    "            data.sort_values(['r_patr_start', 'r_bibl_start', 'patr_stop', 'bibl_stop'], \\\n",
    "                              ascending = [True, True, True, True], inplace=True)\n",
    "        # Delete sort columns\n",
    "        data.drop('r_bibl_start', 1, inplace=True)\n",
    "        data.drop('r_patr_start', 1, inplace=True)\n",
    "        \n",
    "#         print(data)\n",
    "        return data, refListBase, refListComp\n",
    "        \n",
    "                        \n",
    "    def allign(self): #fcmatch_dict is to be changed to the contextualized_match_dict\n",
    "        fcmatch_dict = self.result_dict\n",
    "        collation = Collation()\n",
    "        result_list = []\n",
    "        for sm in fcmatch_dict: # sm = source match = tuple of two documents that have matching ngrams\n",
    "            if sm[0].startswith('TF_'):\n",
    "                api_b = self.tf_apis_dict[sm[0]]\n",
    "                b_min, b_max = api_b.F.otype.s('word')[0], api_b.F.otype.s('word')[-1]\n",
    "            else:\n",
    "                b_min, b_max = 0, len(list(self.base_sources[sm[0]]))\n",
    "               \n",
    "            if sm[1].startswith('TF_'):\n",
    "                api_c = self.tf_apis_dict[sm[1]]\n",
    "                c_min, c_max = api_c.F.otype.s('word')[0], api_c.F.otype.s('word')[-1]\n",
    "            else:\n",
    "                c_min, c_max = 0, len(list(self.comp_sources[sm[1]])) if not self.comp_path == None else len(list(self.base_sources[sm[1]]))\n",
    "            \n",
    "            for match in fcmatch_dict[sm]:\n",
    "#                 print(match)\n",
    "                base_start = (match[0] - self.context) if not (match[0] - self.context) < b_min else b_min\n",
    "                base_stop  = (match[1] + self.context) if not (match[1] + self.context) > b_max else b_max # Make mechanism to measure max length and to test it...\n",
    "                comp_start = (match[2] - self.context) if not (match[2] - self.context) < c_min else c_min\n",
    "                comp_stop  = (match[3] + self.context) if not (match[3] + self.context) > c_max else c_max# Make mechanism to measure max length and to test it...\n",
    "                \n",
    "                if sm[0].startswith('TF_'):\n",
    "                    # Definition of references\n",
    "                    try:\n",
    "                        b_start = [ str(i[1]) for i in api_b.T.headingFromNode(api_b.L.u(match[0]+1, otype=api_b.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    except IndexError:\n",
    "                        b_start = [ str(i[1]) for i in api_b.T.headingFromNode(api_b.L.u(match[0]+1, otype=api_b.F.otype.meta['availableStructure'].split(',')[-2])[0])[1:] ] + ['1']\n",
    "                    try:\n",
    "                        b_stop = [ str(i[1]) for i in api_b.T.headingFromNode(api_b.L.u(match[1], otype=api_b.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    except IndexError:\n",
    "                        b_stop = [ str(i[1]) for i in api_b.T.headingFromNode(api_b.L.u(match[1], otype=api_b.F.otype.meta['availableStructure'].split(',')[-2])[0])[1:] ] + ['1']\n",
    "                    try:\n",
    "                        c_start = [ str(i[1]) for i in api_c.T.headingFromNode(api_c.L.u(match[2]+1, otype=api_c.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    except IndexError:\n",
    "                        c_start = [ str(i[1]) for i in api_c.T.headingFromNode(api_c.L.u(match[2]+1, otype=api_c.F.otype.meta['availableStructure'].split(',')[-2])[0])[1:] ] + ['1']\n",
    "                    try:\n",
    "                        c_stop = [ str(i[1]) for i in api_c.T.headingFromNode(api_c.L.u(match[3], otype=api_c.F.otype.meta['availableStructure'].split(',')[-1])[0])[1:] ]\n",
    "                    except IndexError:\n",
    "                        c_stop = [ str(i[1]) for i in api_c.T.headingFromNode(api_c.L.u(match[3], otype=api_c.F.otype.meta['availableStructure'].split(',')[-2])[0])[1:] ] + ['1']\n",
    "                    \n",
    "                    # The prefixes (base and comp) are necessary, because Collatex uses the reference for the witness names. \n",
    "                    # If a reference is identical, Collatex runs into problems, because it cannot distinguish them anymore...\n",
    "                    fref_base = f'base: {api_b.F.otype.meta[\"_book\"]} {\".\".join(b_start)}' if b_start == b_stop else \\\n",
    "                                f'base: {api_b.F.otype.meta[\"_book\"]} {\".\".join(b_start)}-{b_stop[-1]}' if b_start[:-1] == b_stop[:-1] else \\\n",
    "                                f'base: {api_b.F.otype.meta[\"_book\"]} {\".\".join(b_start)}-{\".\".join(b_stop)}'\n",
    "                    fref_comp = f'comp: {api_c.F.otype.meta[\"_book\"]} {\".\".join(c_start)}' if c_start == c_stop else \\\n",
    "                                f'comp: {api_c.F.otype.meta[\"_book\"]} {\".\".join(c_start)}-{c_stop[-1]}' if c_start[:-1] == c_stop[:-1] else \\\n",
    "                                f'comp: {api_c.F.otype.meta[\"_book\"]} {\".\".join(c_start)}-{\".\".join(c_stop)}'\n",
    "                    \n",
    "                    # Define pretokenized JSON for CollateX\n",
    "                    key_val = \"t\"\n",
    "                    tokens_base_text = [ {key_val: token + \" \"} for token in api_b.T.text(range(base_start+1, base_stop+1), fmt='text-orig-main', descend=True).split() ]\n",
    "                    tokens_comp_text = [ {key_val: token + \" \"} for token in api_c.T.text(range(comp_start+1, comp_stop+1), fmt='text-orig-main', descend=True).split() ]\n",
    "                    witness_base_text = { \"id\": fref_base, \"tokens\": tokens_base_text }\n",
    "                    witness_comp_text = { \"id\": fref_comp, \"tokens\": tokens_comp_text }\n",
    "                    JSON_input = { \"witnesses\": [ witness_base_text, witness_comp_text ] }\n",
    "\n",
    "                elif self.comp_path == None:\n",
    "                    key_val = \"t\"\n",
    "                    tokens_base_text = [ {key_val: token + \" \"} for token in list(self.base_sources[sm[0]])[base_start:base_stop] ]\n",
    "                    tokens_comp_text = [ {key_val: token + \" \"} for token in list(self.base_sources[sm[1]])[comp_start:comp_stop] ]\n",
    "                    witness_base_text = { \"id\": sm[0], \"tokens\": tokens_base_text }\n",
    "                    witness_comp_text = { \"id\": sm[1], \"tokens\": tokens_comp_text }\n",
    "                    JSON_input = { \"witnesses\": [ witness_base_text, witness_comp_text ] }\n",
    "                \n",
    "                else:\n",
    "                    key_val = \"t\"\n",
    "                    tokens_base_text = [ {key_val: token + \" \"} for token in list(self.base_sources[sm[0]])[base_start:base_stop] ]\n",
    "                    tokens_comp_text = [ {key_val: token + \" \"} for token in list(self.comp_sources[sm[1]])[comp_start:comp_stop] ]\n",
    "                    witness_base_text = { \"id\": sm[0], \"tokens\": tokens_base_text }\n",
    "                    witness_comp_text = { \"id\": sm[1], \"tokens\": tokens_comp_text }\n",
    "                    JSON_input = { \"witnesses\": [ witness_base_text, witness_comp_text ] }\n",
    "                    \n",
    "                # Run the collation of Collatex and print the results\n",
    "#               results = collate(JSON_input, output='html', layout=\"horizontal\", segmentation=True)\n",
    "                collate(JSON_input, output='html', layout=\"horizontal\", segmentation=True)\n",
    "        \n",
    "    \n",
    "    def tf_initiate(self, location):\n",
    "        TF = Fabric(locations=location)\n",
    "        api = TF.load('', silent=True)\n",
    "        allFeatures = TF.explore(silent=True, show=True)\n",
    "        loadableFeatures = allFeatures['nodes'] + allFeatures['edges']\n",
    "        TF.load(loadableFeatures, add=True, silent=True)\n",
    "        return api\n",
    "    \n",
    "    \n",
    "    def tf_api_clean(self, base=False, comp=False):\n",
    "        if base == True:\n",
    "            for api in self.base_list:\n",
    "                del api\n",
    "        elif comp == True:\n",
    "            del TF_comp_api\n",
    "        else:\n",
    "            print('Nothing to clean... Please define base=True or comp=True...')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "REPO = '~/github/pthu/patristics'\n",
    "VERSION = '1.0/'\n",
    "TF_DIR = os.path.expanduser(f'{REPO}/tf/{VERSION}')\n",
    "\n",
    "tm = Timestamp()\n",
    "\n",
    "# x = FlexGrams(base_path=TF_DIR + 'new_testament/Brooke Foss Westcott, Fenton John Anthony Hort/New Testament - John',\n",
    "# x = FlexGrams(base_path=normalize('NFD', 'διὰ τὴν ὑπερβολὴν τῆς θαυμασιότητος δοξολογῶν αὐτὸν ὁ ἄγγελος ἐφαίνετο· οἷα λεγειν· σή ἐστιν ἡ ἰσχὺς δέσποτα σὺ γὰρ ἴσχυας κατὰ θανάτου ἐλευθέρωσας τὸ γένος ἀνθρώπων'), \n",
    "# x = FlexGrams(base_path=TF_DIR + 'Alcibiades',\n",
    "x = FlexGrams(base_path=TF_DIR + 'patristics/Clement Of Alexandria/Paedagogus',\n",
    "# x = FlexGrams(base_path=TF_DIR + 'patristics/Clement Of Alexandria/Protrepticus',\n",
    "# x = FlexGrams(base_path=TF_DIR + 'pt/Eusebius/Historia Ecclesiastica',\n",
    "# x = FlexGrams(base_path=TF_DIR + 'new_testament/Brooke Foss Westcott, Fenton John Anthony Hort/New Testament - Matthew',\n",
    "# x = FlexGrams('test',          \n",
    "#            comp_path=TF_DIR + 'plutarch',\n",
    "#         comp_path=TF_DIR + 'patristics/Catenae (Novum Testamentum)',\n",
    "        comp_path=TF_DIR + 'new_testament/Brooke Foss Westcott, Fenton John Anthony Hort/New Testament - Mark',\n",
    "#            comp_path=TF_DIR + 'patristics/Eusebius/Historia Ecclesiastica',   \n",
    "           ngram=6, skip=1, number=1, context=3, distance_base=5, ngram_type='unordered', distance_comp=5, self_match=False, mode=2)\n",
    "x.allign()\n",
    "# x.refResult(order='base', c_levels=(0, 2, 3))\n",
    "tm.info('This is what it takes...')\n",
    "\n",
    "#TODO number should be present in both!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.fabric import Fabric, Timestamp\n",
    "# import os\n",
    "# from pprint import pprint\n",
    "\n",
    "# REPO = '~/github/pthu/patristics'\n",
    "# VERSION = '1.0/'\n",
    "# TF_DIR = os.path.expanduser(f'{REPO}/tf/{VERSION}')\n",
    "\n",
    "# TF = Fabric(locations=TF_DIR + 'patristics/Clement Of Alexandria/Paedagogus')\n",
    "# # TF = Fabric(locations=TF_DIR + 'new_testament/Brooke Foss Westcott, Fenton John Anthony Hort/New Testament - John')\n",
    "# api = TF.load('', silent=False)\n",
    "# allFeatures = TF.explore(silent=True, show=True)\n",
    "# loadableFeatures = allFeatures['nodes'] + allFeatures['edges']\n",
    "# TF.load(loadableFeatures, add=True, silent=True)\n",
    "\n",
    "# # Struct = api.F.otype.meta['availableStructure'].split(',')\n",
    "# # print(Struct)\n",
    "# # api.T.structureInfo()\n",
    "# # print(f'structureFeats = {api.T.structureFeats}')\n",
    "# # print(f'structureTypes = {api.T.structureTypes}')\n",
    "# # print(f'sectionTypes = {api.T.sectionTypes}')\n",
    "# # print(f'structureTypeSet = {api.T.structureTypeSet}')\n",
    "\n",
    "# def reference(wordNode):\n",
    "#     embedTuple = api.L.u(wordNode)\n",
    "#     ref = ['error']\n",
    "#     for node in embedTuple:\n",
    "#         feature = api.F.otype.v(node)\n",
    "#         if feature not in Struct:\n",
    "# #             print(feature)\n",
    "#             continue\n",
    "#         else:\n",
    "#             print(feature)\n",
    "#             heading = api.T.headingFromNode(node)\n",
    "#             print(heading)\n",
    "#             if len(heading) > len(ref):\n",
    "#                 ref = heading\n",
    "#     reference = '.'.join([str(i[1]) for i in ref[:]])\n",
    "#     return reference\n",
    "# # print(reference(48500))\n",
    "    \n",
    "# # verseHeadings = [api.T.headingFromNode(v)[1:] for v in api.F.otype.s(api.F.otype.meta['availableStructure'].split(',')[-1])]\n",
    "# # print(verseHeadings)\n",
    "\n",
    "# # nodeType = api.F.otype.meta['availableStructure'].split(',')[-2]\n",
    "# # nodeType = 'subsection'\n",
    "# # sectionNodes = api.F.otype.s(nodeType)\n",
    "# # sectionList = [api.T.headingFromNode(node)[1:] for node in sectionNodes]\n",
    "# # refList = [\".\".join(str(i) for i in api.T.sectionFromNode(node)[1:]) for node in sectionNodes]\n",
    "# # print(refList)\n",
    "\n",
    "\n",
    "\n",
    "# # node = 11000 # Exemplaria Gratii\n",
    "# # nodeTypes = api.F.otype.meta['availableStructure'].split(',')\n",
    "# # # print(nodeTypes)\n",
    "\n",
    "# # sectionList = \".\".join([ str(api.Fs(level).v( i )) for level in nodeTypes for i in api.L.u(node, otype=level) ])\n",
    "# # print(sectionList)\n",
    "\n",
    "# # #Build refList (with sectionTypes)\n",
    "# # nodeType = api.TF.features['otext'].metaData['sectionTypes'].split(',')[-1]\n",
    "# # sectionNodes = api.F.otype.s(nodeType)\n",
    "# # print(sectionNodes)\n",
    "# # refList = [\".\".join(str(i) for i in api.T.sectionFromNode(node)[1:]) for node in sectionNodes]\n",
    "# # # print(refList)\n",
    "\n",
    "# # #Build refList (with structureTypes)\n",
    "# nodeType = api.TF.features['otext'].metaData['availableStructure'].split(',')[-1]\n",
    "# sectionNodes = api.F.otype.s(nodeType)\n",
    "# # print(sectionNodes)\n",
    "# refList = [\".\".join(str(i[1]) for i in api.T.headingFromNode(node)[1:]) for node in sectionNodes]\n",
    "# refList = [list(str(i[1]) for i in api.T.headingFromNode(node)[1:]) for node in sectionNodes]\n",
    "# # print(refList)\n",
    "# refListMod = ['.'.join([st[0]] + st[2:]) for st in refList]\n",
    "# # print(refListMod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
